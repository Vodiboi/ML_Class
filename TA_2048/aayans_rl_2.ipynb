{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9826810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "from sb3_contrib import MaskablePPO\n",
    "from sb3_contrib.common.maskable.evaluation import evaluate_policy\n",
    "from sb3_contrib.common.maskable.utils import get_action_masks\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "# from sb3_contrib.common.maskable.wrappers import VecActionMasker \n",
    "# from sb3_contrib.common.maskable.vec_env import MaskableEnv\n",
    "# from stable_baselines3.common.vec_env import DummyVecEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37348480",
   "metadata": {},
   "source": [
    "# 2x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8f4fe5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwentyFortyEight2b2Env(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 4}\n",
    "\n",
    "    def __init__(self, render_mode='human'):\n",
    "        super().__init__()\n",
    "        self.render_mode = render_mode\n",
    "        self.observation_space = spaces.Box(0, 10, shape=(2, 2), dtype=np.int8)\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.grid = None\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # this lets gymnasium/SB3 seed properly\n",
    "        super().reset(seed=seed)\n",
    "        self.grid = np.zeros((2, 2), dtype=np.int8)\n",
    "        self._add_tile()\n",
    "        self._add_tile()\n",
    "        return self.grid.copy(), {\"action_mask\": self.valid_action_mask()}\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action), f\"{action} invalid\"\n",
    "        terminated = not self._can_move()\n",
    "        before = self.grid.copy()\n",
    "        reward = self._move_and_merge(action)\n",
    "        if not np.array_equal(before, self.grid):\n",
    "            self._add_tile()\n",
    "        else:\n",
    "            terminated = True\n",
    "        return self.grid.copy(), reward, terminated, False, {\"action_mask\": self.valid_action_mask()}\n",
    "\n",
    "    def render(self):\n",
    "        grd = (np.where(self.grid != 0, 2**self.grid, np.zeros_like(self.grid))).astype(int)\n",
    "        f = lambda x: f\"{x:>2}\" if x != 0 else \"  \"\n",
    "        print(\"-\"*7)\n",
    "        for r in grd:\n",
    "            print(\"|\", end=\"\")\n",
    "            print(*map(f, r), end=\"\")\n",
    "            print(\"|\")\n",
    "        print(\"-\"*7)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    def valid_action_mask(self):\n",
    "        # True = valid, False = would do nothing\n",
    "        mask = []\n",
    "        for a in range(self.action_space.n):\n",
    "            # simulate=True returns reward but *doesnâ€™t* modify self.grid\n",
    "            moved = self._move_and_merge(a, simulate=True) > 0\n",
    "            mask.append(moved)\n",
    "        return np.array(mask, dtype=bool)\n",
    "\n",
    "    def _add_tile(self):\n",
    "        empties = list(zip(*np.where(self.grid == 0)))\n",
    "        if not empties: return\n",
    "        y, x = random.choice(empties)\n",
    "        self.grid[y, x] = 1 if random.random() < 0.9 else 2\n",
    "\n",
    "    def _can_move(self):\n",
    "        if np.any(self.grid == 0):\n",
    "            return True\n",
    "        for a in range(4):\n",
    "            if self._move_and_merge(a, simulate=True) > 0:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def _move_and_merge(self, action, simulate=False):\n",
    "        grid = self.grid.copy() if simulate else self.grid\n",
    "        orig = self.grid.copy()\n",
    "        total_reward = 0\n",
    "        for i in range(2):\n",
    "            line = grid[:, i] if action < 2 else grid[i, :]\n",
    "            if action % 2 == 1:\n",
    "                line = line[::-1]\n",
    "            nonzero = line[line > 0]\n",
    "            merged, skip = [], False\n",
    "            for j in range(len(nonzero)):\n",
    "                if skip:\n",
    "                    skip = False\n",
    "                    continue\n",
    "                if j+1 < len(nonzero) and nonzero[j] == nonzero[j+1]:\n",
    "                    new_val = nonzero[j] + 1\n",
    "                    merged.append(new_val)\n",
    "                    total_reward += 2 ** new_val\n",
    "                    skip = True\n",
    "                else:\n",
    "                    merged.append(nonzero[j])\n",
    "            merged = np.array(merged + [0]*(2-len(merged)), dtype=np.int8)\n",
    "            if action % 2 == 1:\n",
    "                merged = merged[::-1]\n",
    "            if action < 2:\n",
    "                grid[:, i] = merged\n",
    "            else:\n",
    "                grid[i, :] = merged\n",
    "        # add in something to really incentivize not doing nothing\n",
    "        if (grid == orig).all():\n",
    "            total_reward = -2.0\n",
    "        if not simulate:\n",
    "            self.grid = grid\n",
    "        return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "777501de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 5289 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 0    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2361        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014607476 |\n",
      "|    clip_fraction        | 0.252       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.000553    |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 17.8        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0294     |\n",
      "|    value_loss           | 37          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 2021        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019009024 |\n",
      "|    clip_fraction        | 0.275       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.33       |\n",
      "|    explained_variance   | 0.0952      |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 17.5        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0313     |\n",
      "|    value_loss           | 36.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1912        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017857555 |\n",
      "|    clip_fraction        | 0.249       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | 0.0959      |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 16.6        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0353     |\n",
      "|    value_loss           | 42.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1881        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019261433 |\n",
      "|    clip_fraction        | 0.268       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | 0.114       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 39          |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0373     |\n",
      "|    value_loss           | 61.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1862        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021594293 |\n",
      "|    clip_fraction        | 0.273       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | 0.138       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 46.1        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0397     |\n",
      "|    value_loss           | 81          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1848        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023674807 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.162       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 25.6        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0365     |\n",
      "|    value_loss           | 88          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1837        |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022121701 |\n",
      "|    clip_fraction        | 0.233       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.94       |\n",
      "|    explained_variance   | 0.191       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 68.2        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0367     |\n",
      "|    value_loss           | 118         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1829        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025327161 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.85       |\n",
      "|    explained_variance   | 0.158       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 84.1        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.0345     |\n",
      "|    value_loss           | 181         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1822       |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 11         |\n",
      "|    total_timesteps      | 20480      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02220625 |\n",
      "|    clip_fraction        | 0.242      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.763     |\n",
      "|    explained_variance   | 0.245      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 91.7       |\n",
      "|    n_updates            | 270        |\n",
      "|    policy_gradient_loss | -0.0365    |\n",
      "|    value_loss           | 166        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1814        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018071018 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.679      |\n",
      "|    explained_variance   | 0.24        |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 113         |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0329     |\n",
      "|    value_loss           | 223         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1806        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018672176 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.607      |\n",
      "|    explained_variance   | 0.319       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 141         |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0251     |\n",
      "|    value_loss           | 226         |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1798      |\n",
      "|    iterations           | 13        |\n",
      "|    time_elapsed         | 14        |\n",
      "|    total_timesteps      | 26624     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0208298 |\n",
      "|    clip_fraction        | 0.16      |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.556    |\n",
      "|    explained_variance   | 0.384     |\n",
      "|    learning_rate        | 0.001     |\n",
      "|    loss                 | 92.7      |\n",
      "|    n_updates            | 360       |\n",
      "|    policy_gradient_loss | -0.0249   |\n",
      "|    value_loss           | 207       |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1792        |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015538472 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.507      |\n",
      "|    explained_variance   | 0.367       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 176         |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0189     |\n",
      "|    value_loss           | 276         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1790        |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018686805 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.459      |\n",
      "|    explained_variance   | 0.518       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 103         |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0164     |\n",
      "|    value_loss           | 200         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1786        |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024512663 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.442      |\n",
      "|    explained_variance   | 0.39        |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 96.5        |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.0186     |\n",
      "|    value_loss           | 279         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1784        |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022562513 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.416      |\n",
      "|    explained_variance   | 0.441       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 171         |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0204     |\n",
      "|    value_loss           | 295         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1782        |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016873725 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.399      |\n",
      "|    explained_variance   | 0.528       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 164         |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0158     |\n",
      "|    value_loss           | 241         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1781        |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028265972 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.395      |\n",
      "|    explained_variance   | 0.494       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 86.2        |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.017      |\n",
      "|    value_loss           | 286         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1780        |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022022631 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.375      |\n",
      "|    explained_variance   | 0.503       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 131         |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0159     |\n",
      "|    value_loss           | 253         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1778       |\n",
      "|    iterations           | 21         |\n",
      "|    time_elapsed         | 24         |\n",
      "|    total_timesteps      | 43008      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01831564 |\n",
      "|    clip_fraction        | 0.153      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.371     |\n",
      "|    explained_variance   | 0.554      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 79.9       |\n",
      "|    n_updates            | 600        |\n",
      "|    policy_gradient_loss | -0.0138    |\n",
      "|    value_loss           | 213        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1778       |\n",
      "|    iterations           | 22         |\n",
      "|    time_elapsed         | 25         |\n",
      "|    total_timesteps      | 45056      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01863081 |\n",
      "|    clip_fraction        | 0.128      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.355     |\n",
      "|    explained_variance   | 0.525      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 114        |\n",
      "|    n_updates            | 630        |\n",
      "|    policy_gradient_loss | -0.0108    |\n",
      "|    value_loss           | 276        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1777       |\n",
      "|    iterations           | 23         |\n",
      "|    time_elapsed         | 26         |\n",
      "|    total_timesteps      | 47104      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02595669 |\n",
      "|    clip_fraction        | 0.147      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.357     |\n",
      "|    explained_variance   | 0.477      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 136        |\n",
      "|    n_updates            | 660        |\n",
      "|    policy_gradient_loss | -0.0124    |\n",
      "|    value_loss           | 260        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1776        |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029020583 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.339      |\n",
      "|    explained_variance   | 0.545       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 116         |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0139     |\n",
      "|    value_loss           | 241         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1775        |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016808521 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.363      |\n",
      "|    explained_variance   | 0.466       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 77.9        |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0133     |\n",
      "|    value_loss           | 322         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1774        |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025331229 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.355      |\n",
      "|    explained_variance   | 0.552       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 147         |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | -0.0122     |\n",
      "|    value_loss           | 238         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1774        |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025658026 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.338      |\n",
      "|    explained_variance   | 0.452       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 171         |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.0126     |\n",
      "|    value_loss           | 326         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1772        |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 32          |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034167968 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.348      |\n",
      "|    explained_variance   | 0.535       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 99.8        |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.015      |\n",
      "|    value_loss           | 249         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1771        |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 33          |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018828318 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.318      |\n",
      "|    explained_variance   | 0.575       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 118         |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0139     |\n",
      "|    value_loss           | 232         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1771        |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 34          |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023115171 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.319      |\n",
      "|    explained_variance   | 0.563       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 200         |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.0138     |\n",
      "|    value_loss           | 245         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1771        |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 35          |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017350025 |\n",
      "|    clip_fraction        | 0.12        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.317      |\n",
      "|    explained_variance   | 0.546       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 122         |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.0099     |\n",
      "|    value_loss           | 258         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1755        |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 37          |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022397973 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.319      |\n",
      "|    explained_variance   | 0.537       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 147         |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.00905    |\n",
      "|    value_loss           | 258         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1755       |\n",
      "|    iterations           | 33         |\n",
      "|    time_elapsed         | 38         |\n",
      "|    total_timesteps      | 67584      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03024423 |\n",
      "|    clip_fraction        | 0.183      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.326     |\n",
      "|    explained_variance   | 0.611      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 133        |\n",
      "|    n_updates            | 960        |\n",
      "|    policy_gradient_loss | -0.0122    |\n",
      "|    value_loss           | 201        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1755       |\n",
      "|    iterations           | 34         |\n",
      "|    time_elapsed         | 39         |\n",
      "|    total_timesteps      | 69632      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04330265 |\n",
      "|    clip_fraction        | 0.159      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.329     |\n",
      "|    explained_variance   | 0.49       |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 93         |\n",
      "|    n_updates            | 990        |\n",
      "|    policy_gradient_loss | -0.0167    |\n",
      "|    value_loss           | 283        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1755        |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 40          |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023886884 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.307      |\n",
      "|    explained_variance   | 0.593       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 68          |\n",
      "|    n_updates            | 1020        |\n",
      "|    policy_gradient_loss | -0.0118     |\n",
      "|    value_loss           | 202         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1755       |\n",
      "|    iterations           | 36         |\n",
      "|    time_elapsed         | 41         |\n",
      "|    total_timesteps      | 73728      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03220006 |\n",
      "|    clip_fraction        | 0.13       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.304     |\n",
      "|    explained_variance   | 0.525      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 120        |\n",
      "|    n_updates            | 1050       |\n",
      "|    policy_gradient_loss | -0.0138    |\n",
      "|    value_loss           | 265        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1755        |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024094675 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.292      |\n",
      "|    explained_variance   | 0.505       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 109         |\n",
      "|    n_updates            | 1080        |\n",
      "|    policy_gradient_loss | -0.0119     |\n",
      "|    value_loss           | 289         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1755        |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 44          |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023758229 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.279      |\n",
      "|    explained_variance   | 0.566       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 196         |\n",
      "|    n_updates            | 1110        |\n",
      "|    policy_gradient_loss | -0.0143     |\n",
      "|    value_loss           | 230         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1755        |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 45          |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027967958 |\n",
      "|    clip_fraction        | 0.16        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.301      |\n",
      "|    explained_variance   | 0.517       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 166         |\n",
      "|    n_updates            | 1140        |\n",
      "|    policy_gradient_loss | -0.0186     |\n",
      "|    value_loss           | 299         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1755        |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 46          |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021819698 |\n",
      "|    clip_fraction        | 0.138       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.31       |\n",
      "|    explained_variance   | 0.572       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 117         |\n",
      "|    n_updates            | 1170        |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    value_loss           | 246         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1755        |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 47          |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025134023 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.31       |\n",
      "|    explained_variance   | 0.618       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 115         |\n",
      "|    n_updates            | 1200        |\n",
      "|    policy_gradient_loss | -0.0132     |\n",
      "|    value_loss           | 190         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1753        |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 49          |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031018794 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.287      |\n",
      "|    explained_variance   | 0.535       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 149         |\n",
      "|    n_updates            | 1230        |\n",
      "|    policy_gradient_loss | -0.0144     |\n",
      "|    value_loss           | 287         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1750        |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 50          |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026685026 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.312      |\n",
      "|    explained_variance   | 0.567       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 61.6        |\n",
      "|    n_updates            | 1260        |\n",
      "|    policy_gradient_loss | -0.0134     |\n",
      "|    value_loss           | 225         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1749        |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 51          |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021922689 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.285      |\n",
      "|    explained_variance   | 0.588       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 211         |\n",
      "|    n_updates            | 1290        |\n",
      "|    policy_gradient_loss | -0.0131     |\n",
      "|    value_loss           | 234         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1748        |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 52          |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022876069 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.284      |\n",
      "|    explained_variance   | 0.609       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 50.9        |\n",
      "|    n_updates            | 1320        |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    value_loss           | 191         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1744        |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 54          |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021314517 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.285      |\n",
      "|    explained_variance   | 0.521       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 172         |\n",
      "|    n_updates            | 1350        |\n",
      "|    policy_gradient_loss | -0.0106     |\n",
      "|    value_loss           | 282         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1742        |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 55          |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022880478 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.292      |\n",
      "|    explained_variance   | 0.551       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 189         |\n",
      "|    n_updates            | 1380        |\n",
      "|    policy_gradient_loss | -0.00924    |\n",
      "|    value_loss           | 291         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1742        |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 56          |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023146763 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.293      |\n",
      "|    explained_variance   | 0.498       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 187         |\n",
      "|    n_updates            | 1410        |\n",
      "|    policy_gradient_loss | -0.0101     |\n",
      "|    value_loss           | 337         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1738        |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 57          |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027891045 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.27       |\n",
      "|    explained_variance   | 0.592       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 130         |\n",
      "|    n_updates            | 1440        |\n",
      "|    policy_gradient_loss | -0.0114     |\n",
      "|    value_loss           | 214         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1736       |\n",
      "|    iterations           | 50         |\n",
      "|    time_elapsed         | 58         |\n",
      "|    total_timesteps      | 102400     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03294409 |\n",
      "|    clip_fraction        | 0.15       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.266     |\n",
      "|    explained_variance   | 0.635      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 67.2       |\n",
      "|    n_updates            | 1470       |\n",
      "|    policy_gradient_loss | -0.0159    |\n",
      "|    value_loss           | 158        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1735        |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 60          |\n",
      "|    total_timesteps      | 104448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020553177 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.264      |\n",
      "|    explained_variance   | 0.503       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 154         |\n",
      "|    n_updates            | 1500        |\n",
      "|    policy_gradient_loss | -0.0142     |\n",
      "|    value_loss           | 269         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1733        |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 61          |\n",
      "|    total_timesteps      | 106496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017915905 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.263      |\n",
      "|    explained_variance   | 0.532       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 90.7        |\n",
      "|    n_updates            | 1530        |\n",
      "|    policy_gradient_loss | -0.00887    |\n",
      "|    value_loss           | 291         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1734       |\n",
      "|    iterations           | 53         |\n",
      "|    time_elapsed         | 62         |\n",
      "|    total_timesteps      | 108544     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03467265 |\n",
      "|    clip_fraction        | 0.17       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.277     |\n",
      "|    explained_variance   | 0.627      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 92.1       |\n",
      "|    n_updates            | 1560       |\n",
      "|    policy_gradient_loss | -0.016     |\n",
      "|    value_loss           | 169        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1735        |\n",
      "|    iterations           | 54          |\n",
      "|    time_elapsed         | 63          |\n",
      "|    total_timesteps      | 110592      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.051120583 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.256      |\n",
      "|    explained_variance   | 0.459       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 143         |\n",
      "|    n_updates            | 1590        |\n",
      "|    policy_gradient_loss | -0.0128     |\n",
      "|    value_loss           | 344         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1733        |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 64          |\n",
      "|    total_timesteps      | 112640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017828584 |\n",
      "|    clip_fraction        | 0.114       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.26       |\n",
      "|    explained_variance   | 0.516       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 97.5        |\n",
      "|    n_updates            | 1620        |\n",
      "|    policy_gradient_loss | -0.00989    |\n",
      "|    value_loss           | 327         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1733        |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 66          |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016615681 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.255      |\n",
      "|    explained_variance   | 0.571       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 90.6        |\n",
      "|    n_updates            | 1650        |\n",
      "|    policy_gradient_loss | -0.00899    |\n",
      "|    value_loss           | 259         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1734        |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 67          |\n",
      "|    total_timesteps      | 116736      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022488538 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.251      |\n",
      "|    explained_variance   | 0.688       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 77.5        |\n",
      "|    n_updates            | 1680        |\n",
      "|    policy_gradient_loss | -0.0102     |\n",
      "|    value_loss           | 152         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1735        |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 68          |\n",
      "|    total_timesteps      | 118784      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017028863 |\n",
      "|    clip_fraction        | 0.143       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.254      |\n",
      "|    explained_variance   | 0.669       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 104         |\n",
      "|    n_updates            | 1710        |\n",
      "|    policy_gradient_loss | -0.0104     |\n",
      "|    value_loss           | 177         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1735        |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 69          |\n",
      "|    total_timesteps      | 120832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027900454 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.272      |\n",
      "|    explained_variance   | 0.53        |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 145         |\n",
      "|    n_updates            | 1740        |\n",
      "|    policy_gradient_loss | -0.0129     |\n",
      "|    value_loss           | 306         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1736        |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 70          |\n",
      "|    total_timesteps      | 122880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020018177 |\n",
      "|    clip_fraction        | 0.151       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.259      |\n",
      "|    explained_variance   | 0.603       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 141         |\n",
      "|    n_updates            | 1770        |\n",
      "|    policy_gradient_loss | -0.0108     |\n",
      "|    value_loss           | 225         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1737        |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 71          |\n",
      "|    total_timesteps      | 124928      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026413836 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.253      |\n",
      "|    explained_variance   | 0.539       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 88.9        |\n",
      "|    n_updates            | 1800        |\n",
      "|    policy_gradient_loss | -0.0143     |\n",
      "|    value_loss           | 286         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1738       |\n",
      "|    iterations           | 62         |\n",
      "|    time_elapsed         | 73         |\n",
      "|    total_timesteps      | 126976     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02736891 |\n",
      "|    clip_fraction        | 0.123      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.266     |\n",
      "|    explained_variance   | 0.558      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 127        |\n",
      "|    n_updates            | 1830       |\n",
      "|    policy_gradient_loss | -0.0111    |\n",
      "|    value_loss           | 273        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1739       |\n",
      "|    iterations           | 63         |\n",
      "|    time_elapsed         | 74         |\n",
      "|    total_timesteps      | 129024     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02816146 |\n",
      "|    clip_fraction        | 0.163      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.303     |\n",
      "|    explained_variance   | 0.628      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 80.4       |\n",
      "|    n_updates            | 1860       |\n",
      "|    policy_gradient_loss | -0.0158    |\n",
      "|    value_loss           | 190        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1734        |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 75          |\n",
      "|    total_timesteps      | 131072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.046791635 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.3        |\n",
      "|    explained_variance   | 0.598       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 46.4        |\n",
      "|    n_updates            | 1890        |\n",
      "|    policy_gradient_loss | -0.0169     |\n",
      "|    value_loss           | 190         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1733        |\n",
      "|    iterations           | 65          |\n",
      "|    time_elapsed         | 76          |\n",
      "|    total_timesteps      | 133120      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039265018 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.287      |\n",
      "|    explained_variance   | 0.423       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 121         |\n",
      "|    n_updates            | 1920        |\n",
      "|    policy_gradient_loss | -0.0223     |\n",
      "|    value_loss           | 347         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1731        |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 78          |\n",
      "|    total_timesteps      | 135168      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045934927 |\n",
      "|    clip_fraction        | 0.19        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.286      |\n",
      "|    explained_variance   | 0.548       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 117         |\n",
      "|    n_updates            | 1950        |\n",
      "|    policy_gradient_loss | -0.0199     |\n",
      "|    value_loss           | 245         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1731       |\n",
      "|    iterations           | 67         |\n",
      "|    time_elapsed         | 79         |\n",
      "|    total_timesteps      | 137216     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02305829 |\n",
      "|    clip_fraction        | 0.149      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.272     |\n",
      "|    explained_variance   | 0.587      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 26.2       |\n",
      "|    n_updates            | 1980       |\n",
      "|    policy_gradient_loss | -0.0149    |\n",
      "|    value_loss           | 209        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1732        |\n",
      "|    iterations           | 68          |\n",
      "|    time_elapsed         | 80          |\n",
      "|    total_timesteps      | 139264      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041286282 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.269      |\n",
      "|    explained_variance   | 0.499       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 51.8        |\n",
      "|    n_updates            | 2010        |\n",
      "|    policy_gradient_loss | -0.0179     |\n",
      "|    value_loss           | 274         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1733       |\n",
      "|    iterations           | 69         |\n",
      "|    time_elapsed         | 81         |\n",
      "|    total_timesteps      | 141312     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02103509 |\n",
      "|    clip_fraction        | 0.115      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.252     |\n",
      "|    explained_variance   | 0.501      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 99.6       |\n",
      "|    n_updates            | 2040       |\n",
      "|    policy_gradient_loss | -0.0114    |\n",
      "|    value_loss           | 293        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1734        |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 82          |\n",
      "|    total_timesteps      | 143360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023064926 |\n",
      "|    clip_fraction        | 0.121       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.25       |\n",
      "|    explained_variance   | 0.613       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 77.5        |\n",
      "|    n_updates            | 2070        |\n",
      "|    policy_gradient_loss | -0.0115     |\n",
      "|    value_loss           | 205         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1734        |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 83          |\n",
      "|    total_timesteps      | 145408      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027359601 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.261      |\n",
      "|    explained_variance   | 0.489       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 194         |\n",
      "|    n_updates            | 2100        |\n",
      "|    policy_gradient_loss | -0.0136     |\n",
      "|    value_loss           | 331         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1735        |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 84          |\n",
      "|    total_timesteps      | 147456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024618983 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.27       |\n",
      "|    explained_variance   | 0.542       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 165         |\n",
      "|    n_updates            | 2130        |\n",
      "|    policy_gradient_loss | -0.0132     |\n",
      "|    value_loss           | 263         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1736        |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 86          |\n",
      "|    total_timesteps      | 149504      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029459467 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.26       |\n",
      "|    explained_variance   | 0.611       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 99.2        |\n",
      "|    n_updates            | 2160        |\n",
      "|    policy_gradient_loss | -0.0162     |\n",
      "|    value_loss           | 200         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1736        |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 87          |\n",
      "|    total_timesteps      | 151552      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019364621 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.243      |\n",
      "|    explained_variance   | 0.583       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 104         |\n",
      "|    n_updates            | 2190        |\n",
      "|    policy_gradient_loss | -0.011      |\n",
      "|    value_loss           | 219         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1737       |\n",
      "|    iterations           | 75         |\n",
      "|    time_elapsed         | 88         |\n",
      "|    total_timesteps      | 153600     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03908019 |\n",
      "|    clip_fraction        | 0.126      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.249     |\n",
      "|    explained_variance   | 0.6        |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 72.5       |\n",
      "|    n_updates            | 2220       |\n",
      "|    policy_gradient_loss | -0.0158    |\n",
      "|    value_loss           | 217        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1736        |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 89          |\n",
      "|    total_timesteps      | 155648      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026345491 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.261      |\n",
      "|    explained_variance   | 0.517       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 193         |\n",
      "|    n_updates            | 2250        |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    value_loss           | 318         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1733       |\n",
      "|    iterations           | 77         |\n",
      "|    time_elapsed         | 90         |\n",
      "|    total_timesteps      | 157696     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06915663 |\n",
      "|    clip_fraction        | 0.143      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.277     |\n",
      "|    explained_variance   | 0.592      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 51         |\n",
      "|    n_updates            | 2280       |\n",
      "|    policy_gradient_loss | -0.0118    |\n",
      "|    value_loss           | 233        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1732       |\n",
      "|    iterations           | 78         |\n",
      "|    time_elapsed         | 92         |\n",
      "|    total_timesteps      | 159744     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04057846 |\n",
      "|    clip_fraction        | 0.163      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.277     |\n",
      "|    explained_variance   | 0.563      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 128        |\n",
      "|    n_updates            | 2310       |\n",
      "|    policy_gradient_loss | -0.0192    |\n",
      "|    value_loss           | 221        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1732        |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 93          |\n",
      "|    total_timesteps      | 161792      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027693737 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.277      |\n",
      "|    explained_variance   | 0.512       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 228         |\n",
      "|    n_updates            | 2340        |\n",
      "|    policy_gradient_loss | -0.0136     |\n",
      "|    value_loss           | 307         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1731        |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 94          |\n",
      "|    total_timesteps      | 163840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029074375 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.284      |\n",
      "|    explained_variance   | 0.5         |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 224         |\n",
      "|    n_updates            | 2370        |\n",
      "|    policy_gradient_loss | -0.0148     |\n",
      "|    value_loss           | 313         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1731        |\n",
      "|    iterations           | 81          |\n",
      "|    time_elapsed         | 95          |\n",
      "|    total_timesteps      | 165888      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.038696125 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.275      |\n",
      "|    explained_variance   | 0.576       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 186         |\n",
      "|    n_updates            | 2400        |\n",
      "|    policy_gradient_loss | -0.0142     |\n",
      "|    value_loss           | 229         |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 1725      |\n",
      "|    iterations           | 82        |\n",
      "|    time_elapsed         | 97        |\n",
      "|    total_timesteps      | 167936    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0257257 |\n",
      "|    clip_fraction        | 0.145     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.275    |\n",
      "|    explained_variance   | 0.577     |\n",
      "|    learning_rate        | 0.001     |\n",
      "|    loss                 | 136       |\n",
      "|    n_updates            | 2430      |\n",
      "|    policy_gradient_loss | -0.0117   |\n",
      "|    value_loss           | 249       |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1725        |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 98          |\n",
      "|    total_timesteps      | 169984      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026869915 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.274      |\n",
      "|    explained_variance   | 0.56        |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 85.8        |\n",
      "|    n_updates            | 2460        |\n",
      "|    policy_gradient_loss | -0.0144     |\n",
      "|    value_loss           | 257         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1724        |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 99          |\n",
      "|    total_timesteps      | 172032      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022021204 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.271      |\n",
      "|    explained_variance   | 0.579       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 99.2        |\n",
      "|    n_updates            | 2490        |\n",
      "|    policy_gradient_loss | -0.0096     |\n",
      "|    value_loss           | 238         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1724        |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 100         |\n",
      "|    total_timesteps      | 174080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023122676 |\n",
      "|    clip_fraction        | 0.149       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.286      |\n",
      "|    explained_variance   | 0.573       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 87.2        |\n",
      "|    n_updates            | 2520        |\n",
      "|    policy_gradient_loss | -0.0119     |\n",
      "|    value_loss           | 243         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1724       |\n",
      "|    iterations           | 86         |\n",
      "|    time_elapsed         | 102        |\n",
      "|    total_timesteps      | 176128     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03377457 |\n",
      "|    clip_fraction        | 0.149      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.27      |\n",
      "|    explained_variance   | 0.664      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 81.8       |\n",
      "|    n_updates            | 2550       |\n",
      "|    policy_gradient_loss | -0.0127    |\n",
      "|    value_loss           | 167        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1724        |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 103         |\n",
      "|    total_timesteps      | 178176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027431678 |\n",
      "|    clip_fraction        | 0.168       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.287      |\n",
      "|    explained_variance   | 0.671       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 30.3        |\n",
      "|    n_updates            | 2580        |\n",
      "|    policy_gradient_loss | -0.0118     |\n",
      "|    value_loss           | 172         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1724        |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 104         |\n",
      "|    total_timesteps      | 180224      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034996875 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.281      |\n",
      "|    explained_variance   | 0.584       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 52.3        |\n",
      "|    n_updates            | 2610        |\n",
      "|    policy_gradient_loss | -0.0162     |\n",
      "|    value_loss           | 211         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1723       |\n",
      "|    iterations           | 89         |\n",
      "|    time_elapsed         | 105        |\n",
      "|    total_timesteps      | 182272     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04533238 |\n",
      "|    clip_fraction        | 0.167      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.276     |\n",
      "|    explained_variance   | 0.47       |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 95.1       |\n",
      "|    n_updates            | 2640       |\n",
      "|    policy_gradient_loss | -0.0176    |\n",
      "|    value_loss           | 270        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1724        |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 106         |\n",
      "|    total_timesteps      | 184320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.057140455 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.272      |\n",
      "|    explained_variance   | 0.56        |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 101         |\n",
      "|    n_updates            | 2670        |\n",
      "|    policy_gradient_loss | -0.0185     |\n",
      "|    value_loss           | 247         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1724        |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 108         |\n",
      "|    total_timesteps      | 186368      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035266988 |\n",
      "|    clip_fraction        | 0.145       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.263      |\n",
      "|    explained_variance   | 0.588       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 103         |\n",
      "|    n_updates            | 2700        |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    value_loss           | 223         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1724        |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 109         |\n",
      "|    total_timesteps      | 188416      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028456412 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.255      |\n",
      "|    explained_variance   | 0.473       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 80.2        |\n",
      "|    n_updates            | 2730        |\n",
      "|    policy_gradient_loss | -0.0128     |\n",
      "|    value_loss           | 324         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1725        |\n",
      "|    iterations           | 93          |\n",
      "|    time_elapsed         | 110         |\n",
      "|    total_timesteps      | 190464      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034929313 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.249      |\n",
      "|    explained_variance   | 0.562       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 172         |\n",
      "|    n_updates            | 2760        |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    value_loss           | 251         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1725        |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 111         |\n",
      "|    total_timesteps      | 192512      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025744641 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.27       |\n",
      "|    explained_variance   | 0.525       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 119         |\n",
      "|    n_updates            | 2790        |\n",
      "|    policy_gradient_loss | -0.0169     |\n",
      "|    value_loss           | 307         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1725       |\n",
      "|    iterations           | 95         |\n",
      "|    time_elapsed         | 112        |\n",
      "|    total_timesteps      | 194560     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02742108 |\n",
      "|    clip_fraction        | 0.168      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.275     |\n",
      "|    explained_variance   | 0.688      |\n",
      "|    learning_rate        | 0.001      |\n",
      "|    loss                 | 86.7       |\n",
      "|    n_updates            | 2820       |\n",
      "|    policy_gradient_loss | -0.0154    |\n",
      "|    value_loss           | 143        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1724        |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 113         |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033084087 |\n",
      "|    clip_fraction        | 0.163       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.26       |\n",
      "|    explained_variance   | 0.537       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 116         |\n",
      "|    n_updates            | 2850        |\n",
      "|    policy_gradient_loss | -0.0166     |\n",
      "|    value_loss           | 277         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1724        |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 115         |\n",
      "|    total_timesteps      | 198656      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028885014 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.267      |\n",
      "|    explained_variance   | 0.525       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 160         |\n",
      "|    n_updates            | 2880        |\n",
      "|    policy_gradient_loss | -0.0163     |\n",
      "|    value_loss           | 273         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1722        |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 116         |\n",
      "|    total_timesteps      | 200704      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021403076 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.264      |\n",
      "|    explained_variance   | 0.501       |\n",
      "|    learning_rate        | 0.001       |\n",
      "|    loss                 | 230         |\n",
      "|    n_updates            | 2910        |\n",
      "|    policy_gradient_loss | -0.0112     |\n",
      "|    value_loss           | 337         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x323240dc0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class CustomCNN(BaseFeaturesExtractor):\n",
    "#     \"\"\"\n",
    "#     :param observation_space: (gym.Space)\n",
    "#     :param features_dim: (int) Number of features extracted.\n",
    "#         This corresponds to the number of unit for the last layer.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 256):\n",
    "#         super(CustomCNN, self).__init__(observation_space, features_dim)\n",
    "#         # We assume CxHxW images (channels first)\n",
    "#         # Re-ordering will be done by pre-preprocessing or wrapper\n",
    "#         n_input_channels =1\n",
    "#         self.cnn = nn.Sequential(\n",
    "#             nn.Conv2d(n_input_channels, 10, kernel_size=2, stride=1, padding=\"same\"),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(10, 10, kernel_size=2, stride=1, padding=\"same\"),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(10, 10, kernel_size=2, stride=1, padding=\"same\"),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Flatten(start_dim=1),\n",
    "#         )\n",
    "\n",
    "#         # Compute shape by doing one forward pass\n",
    "#         with torch.no_grad():\n",
    "#             n_flatten = self.cnn(\n",
    "#                 torch.as_tensor(observation_space.sample()[None]).float()\n",
    "#             ).shape[0]*self.cnn(\n",
    "#                 torch.as_tensor(observation_space.sample()[None]).float()\n",
    "#             ).shape[1]\n",
    "            \n",
    "#         self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
    "\n",
    "#     def forward(self, observations) -> torch.Tensor:\n",
    "#         obs=torch.tensor(observations).float()\n",
    "#         return self.linear(self.cnn(obs.reshape((obs.shape[0],1,2,2))))\n",
    "\n",
    "class BlockFeaturesExtractor2D(BaseFeaturesExtractor):\n",
    "    \"\"\"\n",
    "    For a 2D Box obs (shape=(H, W)), takes `blocks` as a list of lists of (r,c) tuples.\n",
    "    Each block is flattened to a list of flat indices, one Linear per block â†’ 1 output.\n",
    "    Final features tensor has shape (batch_size, n_blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 observation_space: gym.spaces.Box,\n",
    "                 blocks: list[list[tuple[int,int]]],\n",
    "                 features_dim=0):\n",
    "        # only support 2D Box for now\n",
    "        assert isinstance(observation_space, gym.spaces.Box) and len(observation_space.shape) == 2, \\\n",
    "            \"Only 2D Box obs supported\"\n",
    "        super().__init__(observation_space, features_dim=len(blocks))\n",
    "\n",
    "        H, W = observation_space.shape\n",
    "        flat_size = H * W\n",
    "\n",
    "        # convert each block of (r,c) pairs to flat indices\n",
    "        self.block_indices: list[list[int]] = []\n",
    "        for block in blocks:\n",
    "            idxs = []\n",
    "            for (r, c) in block:\n",
    "                assert 0 <= r < H and 0 <= c < W, f\"Index {(r,c)} out of bounds\"\n",
    "                idxs.append(r * W + c)\n",
    "            self.block_indices.append(idxs)\n",
    "\n",
    "        # one linear per block\n",
    "        self.linears = nn.ModuleList([\n",
    "            nn.Linear(len(idxs), 1)\n",
    "            for idxs in self.block_indices\n",
    "        ])\n",
    "\n",
    "    def forward(self, observations):\n",
    "        # observations: (batch_size, H, W)\n",
    "        batch_size = observations.shape[0]\n",
    "        # flatten spatial dims\n",
    "        obs_flat = observations.view(batch_size, -1)  # (batch_size, H*W)\n",
    "\n",
    "        outs = []\n",
    "        for linear, idxs in zip(self.linears, self.block_indices):\n",
    "            # gather inputs for this block: (batch_size, block_size)\n",
    "            xb = obs_flat[:, idxs]\n",
    "            # project to (batch_size, 1)\n",
    "            yb = linear(xb)\n",
    "            outs.append(yb)\n",
    "\n",
    "        # concatenate to (batch_size, n_blocks)\n",
    "        return torch.cat(outs, dim=1)\n",
    "\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=lambda *args, **kwargs: BlockFeaturesExtractor2D(*args, **kwargs, blocks=[\n",
    "        [\n",
    "            [0, 0],\n",
    "            [0, 1]\n",
    "        ],\n",
    "        [\n",
    "            [0, 0],\n",
    "            [1, 0],\n",
    "            [1, 1]\n",
    "        ],\n",
    "        [\n",
    "            [1, 1],\n",
    "            [0, 0]\n",
    "        ],\n",
    "        [\n",
    "            [1, 1],\n",
    "            [1, 0]\n",
    "        ],\n",
    "        [\n",
    "            [0, 0],\n",
    "            [0, 1],\n",
    "            [1, 0],\n",
    "            [1, 1]\n",
    "        ]\n",
    "    ]),\n",
    "    features_extractor_kwargs=dict(features_dim=4),\n",
    ")\n",
    "# 1) sanity-check env\n",
    "base_env = TwentyFortyEight2b2Env()\n",
    "check_env(base_env, warn=True)                # <-- will raise if your env is non-compliant\n",
    "def make_env2b2():\n",
    "    env = TwentyFortyEight2b2Env(render_mode='human')\n",
    "    return env\n",
    "\n",
    "# 2) vectorize\n",
    "vec_env2b2 = DummyVecEnv([make_env2b2])\n",
    "\n",
    "# 3) train with any SB3 algo\n",
    "model2b2 = PPO(\"MlpPolicy\", vec_env2b2, policy_kwargs=policy_kwargs, verbose=1, n_epochs=30, learning_rate=0.001)\n",
    "# 500_000 with default lr gives a score 300 smth\n",
    "model2b2.learn(total_timesteps=200_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f98f6c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold 75 -> 0 >=, 1000 <\n",
      "4.204 2.0 7.010163 50.0 -2.0\n",
      "Threshold 75 -> 72 >=, 928 <\n",
      "59.864 62.0 26.258665 170.0 -2.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def do_random():\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    k = 0\n",
    "    while not done:\n",
    "        k += 1\n",
    "        # am = get_action_masks(env)\n",
    "        action = [random.randint(0, 3)]\n",
    "        _, reward, done, _ = vec_env2b2.step(action)\n",
    "        total_reward += reward\n",
    "    return total_reward\n",
    "def do_thing():\n",
    "    obs = vec_env2b2.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    k = 0\n",
    "    while not done:\n",
    "        k += 1\n",
    "        # am = get_action_masks(env)\n",
    "        action, _ = model2b2.predict(obs, deterministic=False)\n",
    "        obs, reward, done, _ = vec_env2b2.step(action)\n",
    "        total_reward += reward\n",
    "    return total_reward\n",
    "def do_wfnc(fnc):\n",
    "    l = np.array([fnc() for _ in range(1000)])\n",
    "    plt.hist(l, bins=range(int(np.min(l)), int(np.max(l))+1, 2))\n",
    "    thresh = 75\n",
    "    a = b = 0\n",
    "    for i in l:\n",
    "        if i >= thresh:\n",
    "            a += 1\n",
    "        else:\n",
    "            b += 1\n",
    "    print(f\"Threshold {thresh} -> {a} >=, {b} <\")\n",
    "    print(np.mean(l), np.median(l), np.std(l), np.max(l), np.min(l))\n",
    "do_wfnc(do_random)\n",
    "do_wfnc(do_thing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "18142ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "|    2|\n",
      "| 4   |\n",
      "-------\n",
      "\n",
      "\n",
      "Move 1: up, reward: [0.], total reward: [0.]\n",
      "-------\n",
      "| 4  2|\n",
      "| 4   |\n",
      "-------\n",
      "\n",
      "\n",
      "Move 2: down, reward: [8.], total reward: [8.]\n",
      "-------\n",
      "|    2|\n",
      "| 8  2|\n",
      "-------\n",
      "\n",
      "\n",
      "Move 3: up, reward: [4.], total reward: [12.]\n",
      "-------\n",
      "| 8  4|\n",
      "| 2   |\n",
      "-------\n",
      "\n",
      "\n",
      "Move 4: right, reward: [0.], total reward: [12.]\n",
      "-------\n",
      "| 8  4|\n",
      "| 2  2|\n",
      "-------\n",
      "\n",
      "\n",
      "Move 5: right, reward: [4.], total reward: [16.]\n",
      "-------\n",
      "| 8  4|\n",
      "| 2  4|\n",
      "-------\n",
      "\n",
      "\n",
      "Move 6: up, reward: [8.], total reward: [24.]\n",
      "-------\n",
      "| 8  8|\n",
      "| 2  2|\n",
      "-------\n",
      "\n",
      "\n",
      "Move 7: left, reward: [20.], total reward: [44.]\n",
      "-------\n",
      "|16   |\n",
      "| 4  2|\n",
      "-------\n",
      "\n",
      "\n",
      "Move 8: up, reward: [0.], total reward: [44.]\n",
      "-------\n",
      "|16  2|\n",
      "| 4  2|\n",
      "-------\n",
      "\n",
      "\n",
      "Move 9: down, reward: [4.], total reward: [48.]\n",
      "-------\n",
      "|16  2|\n",
      "| 4  4|\n",
      "-------\n",
      "\n",
      "\n",
      "Move 10: left, reward: [8.], total reward: [56.]\n",
      "-------\n",
      "|16  2|\n",
      "| 8  2|\n",
      "-------\n",
      "\n",
      "\n",
      "Move 11: up, reward: [4.], total reward: [60.]\n",
      "-------\n",
      "|16  4|\n",
      "| 8  2|\n",
      "-------\n",
      "\n",
      "\n",
      "Move 12: up, reward: [-2.], total reward: [58.]\n",
      "-------\n",
      "|     |\n",
      "| 4  2|\n",
      "-------\n",
      "\n",
      "\n",
      "Final score (sum of merges): [58.]\n",
      "-------\n",
      "|     |\n",
      "| 4  2|\n",
      "-------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4) play one game\n",
    "obs = vec_env2b2.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "k = 0\n",
    "dirr = {\n",
    "    0 : \"up\",\n",
    "    1 : \"down\",\n",
    "    2 : \"left\",\n",
    "    3 : \"right\",\n",
    "}\n",
    "vec_env2b2.render()\n",
    "while not done:\n",
    "    k += 1\n",
    "    # am = get_action_masks(env)\n",
    "    action, _ = model2b2.predict(obs, deterministic=True)\n",
    "    obs, reward, done, _ = vec_env2b2.step(action)\n",
    "    total_reward += reward\n",
    "    print(f\"Move {k}: {dirr[action[0]]}, reward: {reward}, total reward: {total_reward}\")\n",
    "    vec_env2b2.render()\n",
    "    # print(action)\n",
    "    time.sleep(1)\n",
    "print(\"Final score (sum of merges):\", total_reward)\n",
    "vec_env2b2.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0f84881f",
   "metadata": {},
   "outputs": [],
   "source": [
    "envv = DummyVecEnv([lambda *args: TwentyFortyEight2b2Env(render_mode='human')])\n",
    "envv.envs[0].grid = np.array([[1, 2], [1, 3]], dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "62d37108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AH [np.int64(2)]\n",
      "AH [np.int64(3), np.int64(2)]\n",
      "-------\n",
      "|    4|\n",
      "| 4  8|\n",
      "-------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "envv.envs[0]._move_and_merge(1)\n",
    "envv.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48325240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_env2b2.envs[0]._move_and_merge()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa426f3",
   "metadata": {},
   "source": [
    "# 4x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7b26cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwentyFortyEightEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 4}\n",
    "\n",
    "    def __init__(self, render_mode='human'):\n",
    "        super().__init__()\n",
    "        self.render_mode = render_mode\n",
    "        self.observation_space = spaces.Box(0, 17, shape=(4, 4), dtype=np.int32)\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.grid = None\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # this lets gymnasium/SB3 seed properly\n",
    "        super().reset(seed=seed)\n",
    "        self.grid = np.zeros((4, 4), dtype=np.int32)\n",
    "        self._add_tile()\n",
    "        self._add_tile()\n",
    "        return self.grid.copy(), {\"action_mask\": self.valid_action_mask()}\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action), f\"{action} invalid\"\n",
    "        before = self.grid.copy()\n",
    "        reward = self._move_and_merge(action)\n",
    "        terminated = not self._can_move()\n",
    "        if not np.array_equal(before, self.grid):\n",
    "            self._add_tile()\n",
    "        return self.grid.copy(), reward, terminated, False, {\"action_mask\": self.valid_action_mask()}\n",
    "\n",
    "    def render(self):\n",
    "        grd = (np.where(self.grid != 0, 2**self.grid.astype(np.int32), np.zeros_like(self.grid))).astype(int)\n",
    "        f = lambda x: f\"{x:>3}\" if x != 0 else \"   \"\n",
    "        print(\"-\"*17)\n",
    "        for r in grd:\n",
    "            print(\"|\", end=\"\")\n",
    "            print(*map(f, r), end=\"\")\n",
    "            print(\"|\")\n",
    "        print(\"-\"*17)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    def valid_action_mask(self):\n",
    "        # True = valid, False = would do nothing\n",
    "        mask = []\n",
    "        for a in range(self.action_space.n):\n",
    "            # simulate=True returns reward but *doesnâ€™t* modify self.grid\n",
    "            moved = self._move_and_merge(a, simulate=True) > 0\n",
    "            mask.append(moved)\n",
    "        return np.array(mask, dtype=bool)\n",
    "\n",
    "    def _add_tile(self):\n",
    "        empties = list(zip(*np.where(self.grid == 0)))\n",
    "        if not empties: return\n",
    "        y, x = random.choice(empties)\n",
    "        self.grid[y, x] = 1 if random.random() < 0.9 else 2\n",
    "\n",
    "    def _can_move(self):\n",
    "        if np.any(self.grid == 0):\n",
    "            return True\n",
    "        for a in range(4):\n",
    "            if self._move_and_merge(a, simulate=True) > 0:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def _move_and_merge(self, action, simulate=False):\n",
    "        grid = self.grid.copy() if simulate else self.grid\n",
    "        orig = self.grid.copy()\n",
    "        total_reward = 0\n",
    "        for i in range(4):\n",
    "            line = grid[:, i] if action < 2 else grid[i, :]\n",
    "            if action % 2 == 1:\n",
    "                line = line[::-1]\n",
    "            nonzero = line[line > 0]\n",
    "            merged, skip = [], False\n",
    "            for j in range(len(nonzero)):\n",
    "                if skip:\n",
    "                    skip = False\n",
    "                    continue\n",
    "                if j+1 < len(nonzero) and nonzero[j] == nonzero[j+1]:\n",
    "                    new_val = nonzero[j] + 1\n",
    "                    merged.append(new_val)\n",
    "                    total_reward += 2**new_val\n",
    "                    skip = True\n",
    "                else:\n",
    "                    merged.append(nonzero[j])\n",
    "            merged = np.array(merged + [0]*(4-len(merged)), dtype=np.int32)\n",
    "            if action % 2 == 1:\n",
    "                merged = merged[::-1]\n",
    "            if action < 2:\n",
    "                grid[:, i] = merged\n",
    "            else:\n",
    "                grid[i, :] = merged\n",
    "        # add in something to really incentivize not doing nothing\n",
    "        if (grid == orig).all():\n",
    "            total_reward = -2.0\n",
    "        if not simulate:\n",
    "            self.grid = grid\n",
    "        return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51d99488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1) sanity-check env\n",
    "# base_env = TwentyFortyEightEnv()\n",
    "# check_env(base_env, warn=True)                # <-- will raise if your env is non-compliant\n",
    "\n",
    "# def make_env():\n",
    "#     env = TwentyFortyEightEnv(render_mode='human')\n",
    "#     return ActionMasker(env, lambda e: e.valid_action_mask())\n",
    "\n",
    "# class MaskableDummyVecEnv(DummyVecEnv):\n",
    "#     def has_attr(self, name: str) -> bool:\n",
    "#         # MaskablePPO will call `env.has_attr(\"action_masks\")`\n",
    "#         return all(hasattr(env, name) for env in self.envs)\n",
    "#     def action_masks(self) -> np.ndarray:\n",
    "#         # must return shape (n_envs, n_actions)\n",
    "#         masks = [env.action_masks() for env in self.envs]\n",
    "#         return np.stack(masks, axis=0)\n",
    "\n",
    "# # 2) vectorize\n",
    "# vec_env = MaskableDummyVecEnv([make_env])\n",
    "\n",
    "\n",
    "# # 3) train with any SB3 algo\n",
    "# model = MaskablePPO(\"MlpPolicy\", vec_env, verbose=1, n_epochs=30, learning_rate=5e-3)\n",
    "# # 500_000 with default lr gives a score 300 smth\n",
    "# model.learn(total_timesteps=100_000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfcb85c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomasdellavigna/Library/Python/3.9/lib/python/site-packages/stable_baselines3/common/env_checker.py:272: UserWarning: Your observation  has an unconventional shape (neither an image, nor a 1D vector). We recommend you to flatten the observation to have only a 1D vector or use a custom policy to properly process the data.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 628  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 3    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 113\u001b[0m\n\u001b[1;32m    110\u001b[0m vec_env \u001b[38;5;241m=\u001b[39m DummyVecEnv([make_env])\n\u001b[1;32m    111\u001b[0m model \u001b[38;5;241m=\u001b[39m PPO(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, vec_env, policy_kwargs\u001b[38;5;241m=\u001b[39mpolicy_kwargs, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, n_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)\n\u001b[0;32m--> 113\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m100000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/stable_baselines3/ppo/ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/stable_baselines3/common/on_policy_algorithm.py:337\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mep_info_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    335\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdump_logs(iteration)\n\u001b[0;32m--> 337\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    339\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/stable_baselines3/ppo/ppo.py:275\u001b[0m, in \u001b[0;36mPPO.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# Optimization step\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 275\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;66;03m# Clip grad norm\u001b[39;00m\n\u001b[1;32m    277\u001b[0m th\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_grad_norm)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "\n",
    "class CustomCNN(BaseFeaturesExtractor):\n",
    "    \"\"\"\n",
    "    :param observation_space: (gym.Space)\n",
    "    :param features_dim: (int) Number of features extracted.\n",
    "        This corresponds to the number of unit for the last layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 160):\n",
    "        super(CustomCNN, self).__init__(observation_space, features_dim)\n",
    "        # We assume CxHxW images (channels first)\n",
    "        # Re-ordering will be done by pre-preprocessing or wrapper\n",
    "        self.dep =30\n",
    "\n",
    "        self.embed= nn.Embedding(20,self.dep)\n",
    "        self.posEm= nn.Embedding(16,self.dep)\n",
    "        self.cnn1 = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(self.dep, self.dep, kernel_size=3, stride=1, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(self.dep, self.dep, kernel_size=3, stride=1, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(self.dep, self.dep, kernel_size=3, stride=1, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm([self.dep,4,4]) )\n",
    "\n",
    "        self.cnn2 = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(self.dep, self.dep, kernel_size=3, stride=1, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(self.dep, self.dep, kernel_size=3, stride=1, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(self.dep, self.dep, kernel_size=3, stride=1, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm([self.dep,4,4]) )\n",
    "        self.cnn3 = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(self.dep, self.dep, kernel_size=3, stride=1, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(self.dep, self.dep, kernel_size=3, stride=1, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(self.dep, self.dep, kernel_size=3, stride=1, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm([self.dep,4,4]))\n",
    "\n",
    "        self.cnn4 = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(self.dep, self.dep//4*3, kernel_size=3, stride=1, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(self.dep//4*3, self.dep//2, kernel_size=3, stride=1, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(self.dep//2, self.dep//4, kernel_size=3, stride=1, padding=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm([self.dep//4,4,4]),\n",
    "            nn.Flatten(start_dim=1))\n",
    "\n",
    "        n_flatten = self.dep//4*16\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(n_flatten, 32*2), nn.ReLU(),\n",
    "            # nn.Linear(50, 50), nn.ReLU(),\n",
    "            nn.Linear(32*2, features_dim), nn.ReLU(),\n",
    "            )\n",
    "        self.to(device=device)\n",
    "    def do_cnn(self, input_1):\n",
    "        res=self.cnn1(input_1)+input_1\n",
    "        res=self.cnn2(res)+res\n",
    "        return self.cnn4(self.cnn3(res)+res)\n",
    "\n",
    "    def do_embed(self, observations):\n",
    "        observations=observations.to(device=device)\n",
    "        add_pos=self.posEm(torch.tensor([i for i in range(16)], dtype=torch.int,device=device)).reshape((4,4,self.dep))\n",
    "        obs=observations.type(torch.int)\n",
    "        return(self.embed(obs)+add_pos).reshape((observations.shape[0],self.dep,4,4))\n",
    "\n",
    "\n",
    "    def forward(self, observations) -> torch.Tensor:\n",
    "        embed= self.do_embed(observations)\n",
    "        final1=self.linear(self.do_cnn(embed))\n",
    "        # final2= self.linear(self.cnn( torch.rot90( embed,1,[2,3])))\n",
    "        # final3= self.linear(self.cnn( torch.rot90( embed,2,[2,3])))\n",
    "        # final4= self.linear(self.cnn( torch.rot90( embed,3,[2,3])))\n",
    "\n",
    "        # print(torch.concat( (final1,final2,final3,final4)).shape,\"Hi\",final1.shape)\n",
    "        # return torch.concat( (final1,final2,final3,final4),dim=1)\n",
    "        return final1\n",
    "\n",
    "# 1) sanity-check env\n",
    "\n",
    "# 2) vectorize\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=CustomCNN,\n",
    "    features_extractor_kwargs=dict(features_dim=64),\n",
    "    net_arch=[64, 64]\n",
    ")\n",
    "\n",
    "base_env = TwentyFortyEightEnv()\n",
    "check_env(base_env, warn=True)                # <-- will raise if your env is non-compliant\n",
    "\n",
    "def make_env():\n",
    "    env = TwentyFortyEightEnv(render_mode='human')\n",
    "    return env\n",
    "\n",
    "vec_env = DummyVecEnv([make_env])\n",
    "model = PPO(\"MlpPolicy\", vec_env, policy_kwargs=policy_kwargs, verbose=1, n_epochs=100, batch_size=256)\n",
    "\n",
    "model.learn(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f5bc27bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thomasdellavigna/Library/Python/3.9/lib/python/site-packages/stable_baselines3/common/env_checker.py:272: UserWarning: Your observation  has an unconventional shape (neither an image, nor a 1D vector). We recommend you to flatten the observation to have only a 1D vector or use a custom policy to properly process the data.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 1) sanity-check env\n",
    "base_env = TwentyFortyEightEnv()\n",
    "check_env(base_env, warn=True)                # <-- will raise if your env is non-compliant\n",
    "\n",
    "def make_env():\n",
    "    env = TwentyFortyEightEnv(render_mode='human')\n",
    "    return env\n",
    "def make_env_play():\n",
    "    env = TwentyFortyEightEnv_play(render_mode='human')\n",
    "    return env\n",
    "# 2) vectorize\n",
    "# vec_env = DummyVecEnv([make_env])\n",
    "\n",
    "vec_env_play = DummyVecEnv([make_env_play])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35d6787",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aayanarish/Library/Python/3.10/lib/python/site-packages/stable_baselines3/common/env_checker.py:272: UserWarning: Your observation  has an unconventional shape (neither an image, nor a 1D vector). We recommend you to flatten the observation to have only a 1D vector or use a custom policy to properly process the data.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Policy MlpLstmPolicy unknown",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m vec_env \u001b[38;5;241m=\u001b[39m DummyVecEnv([make_env])\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# 3) train with any SB3 algo\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPPO\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMlpLstmPolicy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvec_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 500_000 with default lr gives a score 300 smth\u001b[39;00m\n\u001b[1;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39mlearn(total_timesteps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200_000\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/stable_baselines3/ppo/ppo.py:109\u001b[0m, in \u001b[0;36mPPO.__init__\u001b[0;34m(self, policy, env, learning_rate, n_steps, batch_size, n_epochs, gamma, gae_lambda, clip_range, clip_range_vf, normalize_advantage, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, rollout_buffer_class, rollout_buffer_kwargs, target_kl, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     82\u001b[0m     policy: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mtype\u001b[39m[ActorCriticPolicy]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m     _init_setup_model: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    108\u001b[0m ):\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgae_lambda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgae_lambda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43ment_coef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ment_coef\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvf_coef\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvf_coef\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_grad_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_sde\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_sde\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43msde_sample_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msde_sample_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrollout_buffer_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrollout_buffer_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrollout_buffer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrollout_buffer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstats_window_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstats_window_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensorboard_log\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensorboard_log\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_init_setup_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m        \u001b[49m\u001b[43msupported_action_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m            \u001b[49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBox\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m            \u001b[49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDiscrete\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m            \u001b[49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMultiDiscrete\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m            \u001b[49m\u001b[43mspaces\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMultiBinary\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# Sanity check, otherwise it will lead to noisy gradient and NaN\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# because of the advantage normalization\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m normalize_advantage:\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/stable_baselines3/common/on_policy_algorithm.py:86\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.__init__\u001b[0;34m(self, policy, env, learning_rate, n_steps, gamma, gae_lambda, ent_coef, vf_coef, max_grad_norm, use_sde, sde_sample_freq, rollout_buffer_class, rollout_buffer_kwargs, stats_window_size, tensorboard_log, monitor_wrapper, policy_kwargs, verbose, seed, device, _init_setup_model, supported_action_spaces)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     63\u001b[0m     policy: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mtype\u001b[39m[ActorCriticPolicy]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m     supported_action_spaces: Optional[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mtype\u001b[39m[spaces\u001b[38;5;241m.\u001b[39mSpace], \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     85\u001b[0m ):\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_sde\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_sde\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43msde_sample_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msde_sample_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43msupport_multi_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmonitor_wrapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmonitor_wrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstats_window_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstats_window_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtensorboard_log\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensorboard_log\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43msupported_action_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msupported_action_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_steps \u001b[38;5;241m=\u001b[39m n_steps\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m=\u001b[39m gamma\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/stable_baselines3/common/base_class.py:124\u001b[0m, in \u001b[0;36mBaseAlgorithm.__init__\u001b[0;34m(self, policy, env, learning_rate, policy_kwargs, stats_window_size, tensorboard_log, verbose, device, support_multi_env, monitor_wrapper, seed, use_sde, sde_sample_freq, supported_action_spaces)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    108\u001b[0m     policy: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mtype\u001b[39m[BasePolicy]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    121\u001b[0m     supported_action_spaces: Optional[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mtype\u001b[39m[spaces\u001b[38;5;241m.\u001b[39mSpace], \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    122\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(policy, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 124\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_policy_from_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_class \u001b[38;5;241m=\u001b[39m policy\n",
      "File \u001b[0;32m~/Library/Python/3.10/lib/python/site-packages/stable_baselines3/common/base_class.py:339\u001b[0m, in \u001b[0;36mBaseAlgorithm._get_policy_from_name\u001b[0;34m(self, policy_name)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_aliases[policy_name]\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 339\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPolicy \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpolicy_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m unknown\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Policy MlpLstmPolicy unknown"
     ]
    }
   ],
   "source": [
    "#rtain\n",
    "model = PPO(\"MlpLstmPolicy\", vec_env, verbose=1, n_epochs=30, learning_rate=5e-3)\n",
    "# 500_000 with default lr gives a score 300 smth\n",
    "model.learn(total_timesteps=200_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ee31bc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zy/tjrmvbjx24dcxfzdxg41rr400000gp/T/ipykernel_77553/2153850915.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  obs=torch.tensor(observations, dtype=int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Move 1: left, reward: 4, total reward: 4\n",
      "-----------------\n",
      "|          2    |\n",
      "|               |\n",
      "|  4            |\n",
      "|               |\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 2: down, reward: 0, total reward: 4\n",
      "-----------------\n",
      "|               |\n",
      "|          2    |\n",
      "|               |\n",
      "|  4       2    |\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 3: down, reward: 4, total reward: 8\n",
      "-----------------\n",
      "|               |\n",
      "|               |\n",
      "|  2            |\n",
      "|  4       4    |\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 4: left, reward: 8, total reward: 16\n",
      "-----------------\n",
      "|  2            |\n",
      "|               |\n",
      "|  2            |\n",
      "|  8            |\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 5: left, reward: 0, total reward: 16\n",
      "-----------------\n",
      "|  2            |\n",
      "|               |\n",
      "|  2            |\n",
      "|  8            |\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 6: left, reward: 0, total reward: 16\n",
      "-----------------\n",
      "|  2            |\n",
      "|               |\n",
      "|  2            |\n",
      "|  8            |\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 7: right, reward: 0, total reward: 16\n",
      "-----------------\n",
      "|              2|\n",
      "|      4        |\n",
      "|              2|\n",
      "|              8|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 8: down, reward: 4, total reward: 20\n",
      "-----------------\n",
      "|               |\n",
      "|               |\n",
      "|  2           4|\n",
      "|      4       8|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 9: left, reward: 0, total reward: 20\n",
      "-----------------\n",
      "|              2|\n",
      "|               |\n",
      "|  2   4        |\n",
      "|  4   8        |\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 10: right, reward: 0, total reward: 20\n",
      "-----------------\n",
      "|  2           2|\n",
      "|               |\n",
      "|          2   4|\n",
      "|          4   8|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 11: left, reward: 4, total reward: 24\n",
      "-----------------\n",
      "|  4            |\n",
      "|  2            |\n",
      "|  2   4        |\n",
      "|  4   8        |\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 12: right, reward: 0, total reward: 24\n",
      "-----------------\n",
      "|              4|\n",
      "|              2|\n",
      "|  4       2   4|\n",
      "|          4   8|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 13: left, reward: 0, total reward: 24\n",
      "-----------------\n",
      "|  4       2    |\n",
      "|  2            |\n",
      "|  4   2   4    |\n",
      "|  4   8        |\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 14: right, reward: 0, total reward: 24\n",
      "-----------------\n",
      "|          4   2|\n",
      "|          4   2|\n",
      "|      4   2   4|\n",
      "|          4   8|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 15: down, reward: 12, total reward: 36\n",
      "-----------------\n",
      "|               |\n",
      "|  2       8   4|\n",
      "|          2   4|\n",
      "|      4   4   8|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 16: up, reward: 8, total reward: 44\n",
      "-----------------\n",
      "|  2   4   8   8|\n",
      "|          2   8|\n",
      "|          4    |\n",
      "|          2    |\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 17: down, reward: 16, total reward: 60\n",
      "-----------------\n",
      "|          8    |\n",
      "|          2    |\n",
      "|      2   4    |\n",
      "|  2   4   2  16|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 18: up, reward: 0, total reward: 60\n",
      "-----------------\n",
      "|  2   2   8  16|\n",
      "|      4   2    |\n",
      "|      4   4    |\n",
      "|          2    |\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 19: up, reward: 8, total reward: 68\n",
      "-----------------\n",
      "|  2   2   8  16|\n",
      "|      8   2    |\n",
      "|  2       4    |\n",
      "|          2    |\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 20: left, reward: 4, total reward: 72\n",
      "-----------------\n",
      "|  4   8  16    |\n",
      "|  8   2   2    |\n",
      "|  2   4        |\n",
      "|  2            |\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 21: down, reward: 4, total reward: 76\n",
      "-----------------\n",
      "|               |\n",
      "|  4   8       2|\n",
      "|  8   2  16    |\n",
      "|  4   4   2    |\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 22: down, reward: 0, total reward: 76\n",
      "-----------------\n",
      "|          2    |\n",
      "|  4   8        |\n",
      "|  8   2  16    |\n",
      "|  4   4   2   2|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 23: down, reward: 0, total reward: 76\n",
      "-----------------\n",
      "|               |\n",
      "|  4   8   2    |\n",
      "|  8   2  16   2|\n",
      "|  4   4   2   2|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 24: left, reward: 12, total reward: 88\n",
      "-----------------\n",
      "|               |\n",
      "|  4   8   2    |\n",
      "|  8   2  16   2|\n",
      "|  8   4   2    |\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 25: right, reward: 0, total reward: 88\n",
      "-----------------\n",
      "|  2            |\n",
      "|      4   8   2|\n",
      "|  8   2  16   2|\n",
      "|      8   4   2|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 26: down, reward: 4, total reward: 92\n",
      "-----------------\n",
      "|              2|\n",
      "|      4   8    |\n",
      "|  2   2  16   2|\n",
      "|  8   8   4   4|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 27: up, reward: 4, total reward: 96\n",
      "-----------------\n",
      "|  2   4   8   4|\n",
      "|  8   2  16   4|\n",
      "|      8   4    |\n",
      "|              2|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 28: right, reward: 0, total reward: 96\n",
      "-----------------\n",
      "|  2   4   8   4|\n",
      "|  8   2  16   4|\n",
      "|          8   4|\n",
      "|      2       2|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 29: left, reward: 4, total reward: 100\n",
      "-----------------\n",
      "|  2   4   8   4|\n",
      "|  8   2  16   4|\n",
      "|  8   4        |\n",
      "|  4           2|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 30: up, reward: 24, total reward: 124\n",
      "-----------------\n",
      "|  2   4   8   8|\n",
      "| 16   2  16   2|\n",
      "|  4   4        |\n",
      "|          2    |\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 31: left, reward: 24, total reward: 148\n",
      "-----------------\n",
      "|  2   4  16    |\n",
      "| 16   2  16   2|\n",
      "|  8            |\n",
      "|  2       2    |\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 32: right, reward: 4, total reward: 152\n",
      "-----------------\n",
      "|  2   2   4  16|\n",
      "| 16   2  16   2|\n",
      "|              8|\n",
      "|              4|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 33: down, reward: 4, total reward: 156\n",
      "-----------------\n",
      "|             16|\n",
      "|      2       2|\n",
      "|  2       4   8|\n",
      "| 16   4  16   4|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 34: right, reward: 4, total reward: 160\n",
      "-----------------\n",
      "|          2  16|\n",
      "|              4|\n",
      "|      2   4   8|\n",
      "| 16   4  16   4|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 35: left, reward: 0, total reward: 160\n",
      "-----------------\n",
      "|  2  16   2    |\n",
      "|  4            |\n",
      "|  2   4   8    |\n",
      "| 16   4  16   4|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 36: right, reward: 0, total reward: 160\n",
      "-----------------\n",
      "|      2  16   2|\n",
      "|      2       4|\n",
      "|      2   4   8|\n",
      "| 16   4  16   4|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 37: left, reward: 0, total reward: 160\n",
      "-----------------\n",
      "|  2  16   2    |\n",
      "|  2   4        |\n",
      "|  2   4   8   2|\n",
      "| 16   4  16   4|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 38: up, reward: 12, total reward: 172\n",
      "-----------------\n",
      "|  4  16   2   2|\n",
      "|  2   8   8   4|\n",
      "| 16   4  16    |\n",
      "|              2|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 39: right, reward: 20, total reward: 192\n",
      "-----------------\n",
      "|      4  16   4|\n",
      "|      2  16   4|\n",
      "|     16   4  16|\n",
      "|      2       2|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 40: down, reward: 40, total reward: 232\n",
      "-----------------\n",
      "|      4       2|\n",
      "|      2       8|\n",
      "|     16  32  16|\n",
      "|      2   4   2|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 41: left, reward: 0, total reward: 232\n",
      "-----------------\n",
      "|  4   2        |\n",
      "|  2   8        |\n",
      "| 16  32  16    |\n",
      "|  2   4   2   2|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 42: right, reward: 4, total reward: 236\n",
      "-----------------\n",
      "|          4   2|\n",
      "|      2   2   8|\n",
      "|     16  32  16|\n",
      "|      2   4   4|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 43: left, reward: 12, total reward: 248\n",
      "-----------------\n",
      "|  4   2   2    |\n",
      "|  4   8        |\n",
      "| 16  32  16    |\n",
      "|  2   8        |\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 44: right, reward: 4, total reward: 252\n",
      "-----------------\n",
      "|  2       4   4|\n",
      "|          4   8|\n",
      "|     16  32  16|\n",
      "|          2   8|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 45: left, reward: 8, total reward: 260\n",
      "-----------------\n",
      "|  2   8        |\n",
      "|  4   8        |\n",
      "| 16  32  16   2|\n",
      "|  2   8        |\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 46: up, reward: 16, total reward: 276\n",
      "-----------------\n",
      "|  2  16  16   2|\n",
      "|  4  32        |\n",
      "| 16   8   2    |\n",
      "|  2            |\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 47: right, reward: 32, total reward: 308\n",
      "-----------------\n",
      "|  2   2  32   2|\n",
      "|          4  32|\n",
      "|     16   8   2|\n",
      "|              2|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 48: up, reward: 4, total reward: 312\n",
      "-----------------\n",
      "|  2   2  32   2|\n",
      "|  2  16   4  32|\n",
      "|          8   4|\n",
      "|               |\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 49: down, reward: 4, total reward: 316\n",
      "-----------------\n",
      "|               |\n",
      "|         32   2|\n",
      "|  2   2   4  32|\n",
      "|  4  16   8   4|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 50: down, reward: 0, total reward: 316\n",
      "-----------------\n",
      "|               |\n",
      "|         32   2|\n",
      "|  2   2   4  32|\n",
      "|  4  16   8   4|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 51: up, reward: 0, total reward: 316\n",
      "-----------------\n",
      "|  2   2  32   2|\n",
      "|  4  16   4  32|\n",
      "|          8   4|\n",
      "|  2            |\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 52: left, reward: 4, total reward: 320\n",
      "-----------------\n",
      "|  4  32   2    |\n",
      "|  4  16   4  32|\n",
      "|  8   4        |\n",
      "|  2       2    |\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 53: left, reward: 4, total reward: 324\n",
      "-----------------\n",
      "|  4  32   2    |\n",
      "|  4  16   4  32|\n",
      "|  8   4   2    |\n",
      "|  4            |\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 54: down, reward: 8, total reward: 332\n",
      "-----------------\n",
      "|              2|\n",
      "|  8  32   2    |\n",
      "|  8  16   4    |\n",
      "|  4   4   2  32|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 55: down, reward: 16, total reward: 348\n",
      "-----------------\n",
      "|               |\n",
      "|     32   2   2|\n",
      "| 16  16   4   2|\n",
      "|  4   4   2  32|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 56: left, reward: 44, total reward: 392\n",
      "-----------------\n",
      "|              2|\n",
      "| 32   4        |\n",
      "| 32   4   2    |\n",
      "|  8   2  32    |\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 57: down, reward: 72, total reward: 464\n",
      "-----------------\n",
      "|      2        |\n",
      "|               |\n",
      "| 64   8   2    |\n",
      "|  8   2  32   2|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 58: left, reward: 0, total reward: 464\n",
      "-----------------\n",
      "|  2            |\n",
      "|  2            |\n",
      "| 64   8   2    |\n",
      "|  8   2  32   2|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 59: right, reward: 0, total reward: 464\n",
      "-----------------\n",
      "|              2|\n",
      "|  2           2|\n",
      "|     64   8   2|\n",
      "|  8   2  32   2|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 60: right, reward: 4, total reward: 468\n",
      "-----------------\n",
      "|              2|\n",
      "|          2   4|\n",
      "|     64   8   2|\n",
      "|  8   2  32   2|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 61: right, reward: 0, total reward: 468\n",
      "-----------------\n",
      "|              2|\n",
      "|          2   4|\n",
      "|     64   8   2|\n",
      "|  8   2  32   2|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 62: down, reward: 4, total reward: 472\n",
      "-----------------\n",
      "|               |\n",
      "|          2   2|\n",
      "|  2  64   8   4|\n",
      "|  8   2  32   4|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 63: down, reward: 8, total reward: 480\n",
      "-----------------\n",
      "|               |\n",
      "|          2   2|\n",
      "|  2  64   8   2|\n",
      "|  8   2  32   8|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 64: down, reward: 4, total reward: 484\n",
      "-----------------\n",
      "|  2            |\n",
      "|          2    |\n",
      "|  2  64   8   4|\n",
      "|  8   2  32   8|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 65: left, reward: 0, total reward: 484\n",
      "-----------------\n",
      "|  2            |\n",
      "|  2           2|\n",
      "|  2  64   8   4|\n",
      "|  8   2  32   8|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 66: down, reward: 4, total reward: 488\n",
      "-----------------\n",
      "|               |\n",
      "|  2   4       2|\n",
      "|  4  64   8   4|\n",
      "|  8   2  32   8|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 67: up, reward: 0, total reward: 488\n",
      "-----------------\n",
      "|  2   4   8   2|\n",
      "|  4  64  32   4|\n",
      "|  8   2       8|\n",
      "|              4|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 68: right, reward: 0, total reward: 488\n",
      "-----------------\n",
      "|  2   4   8   2|\n",
      "|  4  64  32   4|\n",
      "|      8   2   8|\n",
      "|  2           4|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 69: up, reward: 0, total reward: 488\n",
      "-----------------\n",
      "|  2   4   8   2|\n",
      "|  4  64  32   4|\n",
      "|  2   8   2   8|\n",
      "|      2       4|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 70: left, reward: 0, total reward: 488\n",
      "-----------------\n",
      "|  2   4   8   2|\n",
      "|  4  64  32   4|\n",
      "|  2   8   2   8|\n",
      "|  2   4   2    |\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 71: down, reward: 8, total reward: 496\n",
      "-----------------\n",
      "|      4   4    |\n",
      "|  2  64   8   2|\n",
      "|  4   8  32   4|\n",
      "|  4   4   4   8|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 72: down, reward: 8, total reward: 504\n",
      "-----------------\n",
      "|  2   4   4    |\n",
      "|     64   8   2|\n",
      "|  2   8  32   4|\n",
      "|  8   4   4   8|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 73: up, reward: 4, total reward: 508\n",
      "-----------------\n",
      "|  4   4   4   2|\n",
      "|  8  64   8   4|\n",
      "|  2   8  32   8|\n",
      "|      4   4    |\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 74: up, reward: 0, total reward: 508\n",
      "-----------------\n",
      "|  4   4   4   2|\n",
      "|  8  64   8   4|\n",
      "|  2   8  32   8|\n",
      "|      4   4    |\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 75: down, reward: 0, total reward: 508\n",
      "-----------------\n",
      "|  2   4   4    |\n",
      "|  4  64   8   2|\n",
      "|  8   8  32   4|\n",
      "|  2   4   4   8|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 76: up, reward: 0, total reward: 508\n",
      "-----------------\n",
      "|  2   4   4   2|\n",
      "|  4  64   8   4|\n",
      "|  8   8  32   8|\n",
      "|  2   4   4   2|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 77: left, reward: 32, total reward: 540\n",
      "-----------------\n",
      "|  2   8   2    |\n",
      "|  4  64   8   4|\n",
      "| 16  32   8    |\n",
      "|  2   8   2   2|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 78: up, reward: 16, total reward: 556\n",
      "-----------------\n",
      "|  2   8   2   4|\n",
      "|  4  64  16   2|\n",
      "| 16  32   2    |\n",
      "|  2   8       2|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 79: left, reward: 0, total reward: 556\n",
      "-----------------\n",
      "|  2   8   2   4|\n",
      "|  4  64  16   2|\n",
      "| 16  32   2    |\n",
      "|  2   8   2   2|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 80: down, reward: 8, total reward: 564\n",
      "-----------------\n",
      "|  2   8   2    |\n",
      "|  4  64   2    |\n",
      "| 16  32  16   4|\n",
      "|  2   8   4   4|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 81: left, reward: 8, total reward: 572\n",
      "-----------------\n",
      "|  2   8   2    |\n",
      "|  4  64   2    |\n",
      "| 16  32  16   4|\n",
      "|  2   8   8   2|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 82: up, reward: 4, total reward: 576\n",
      "-----------------\n",
      "|  2   8   4   4|\n",
      "|  4  64  16   2|\n",
      "| 16  32   8   2|\n",
      "|  2   8        |\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 83: up, reward: 4, total reward: 580\n",
      "-----------------\n",
      "|  2   8   4   4|\n",
      "|  4  64  16   4|\n",
      "| 16  32   8   2|\n",
      "|  2   8        |\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 84: down, reward: 8, total reward: 588\n",
      "-----------------\n",
      "|  2   8        |\n",
      "|  4  64   4   2|\n",
      "| 16  32  16   8|\n",
      "|  2   8   8   2|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 85: up, reward: 0, total reward: 588\n",
      "-----------------\n",
      "|  2   8   4   2|\n",
      "|  4  64  16   8|\n",
      "| 16  32   8   2|\n",
      "|  2   8       2|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 86: down, reward: 4, total reward: 592\n",
      "-----------------\n",
      "|  2   8       4|\n",
      "|  4  64   4   2|\n",
      "| 16  32  16   8|\n",
      "|  2   8   8   4|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 87: right, reward: 16, total reward: 608\n",
      "-----------------\n",
      "|  2   2   8   4|\n",
      "|  4  64   4   2|\n",
      "| 16  32  16   8|\n",
      "|      2  16   4|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 88: right, reward: 4, total reward: 612\n",
      "-----------------\n",
      "|      4   8   4|\n",
      "|  4  64   4   2|\n",
      "| 16  32  16   8|\n",
      "|  2   2  16   4|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 89: left, reward: 4, total reward: 616\n",
      "-----------------\n",
      "|  4   8   4   2|\n",
      "|  4  64   4   2|\n",
      "| 16  32  16   8|\n",
      "|  4  16   4    |\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 90: down, reward: 20, total reward: 636\n",
      "-----------------\n",
      "|      8       2|\n",
      "|  8  64   8    |\n",
      "| 16  32  16   4|\n",
      "|  4  16   4   8|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 91: left, reward: 0, total reward: 636\n",
      "-----------------\n",
      "|  8   2       4|\n",
      "|  8  64   8    |\n",
      "| 16  32  16   4|\n",
      "|  4  16   4   8|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 92: down, reward: 24, total reward: 660\n",
      "-----------------\n",
      "|  2   2        |\n",
      "| 16  64   8    |\n",
      "| 16  32  16   8|\n",
      "|  4  16   4   8|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 93: down, reward: 48, total reward: 708\n",
      "-----------------\n",
      "|      2   2    |\n",
      "|  2  64   8    |\n",
      "| 32  32  16    |\n",
      "|  4  16   4  16|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 94: left, reward: 68, total reward: 776\n",
      "-----------------\n",
      "|  4   2        |\n",
      "|  2  64   8    |\n",
      "| 64  16        |\n",
      "|  4  16   4  16|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 95: left, reward: 0, total reward: 776\n",
      "-----------------\n",
      "|  4   2        |\n",
      "|  2  64   8    |\n",
      "| 64  16        |\n",
      "|  4  16   4  16|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 96: up, reward: 32, total reward: 808\n",
      "-----------------\n",
      "|  4   2   8  16|\n",
      "|  2  64   4    |\n",
      "| 64  32        |\n",
      "|  4       2    |\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 97: down, reward: 0, total reward: 808\n",
      "-----------------\n",
      "|  4   2        |\n",
      "|  2   2   8    |\n",
      "| 64  64   4    |\n",
      "|  4  32   2  16|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 98: up, reward: 4, total reward: 812\n",
      "-----------------\n",
      "|  4   4   8  16|\n",
      "|  2  64   4    |\n",
      "| 64  32   2    |\n",
      "|  4           2|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 99: left, reward: 8, total reward: 820\n",
      "-----------------\n",
      "|  8   8  16    |\n",
      "|  2  64   4    |\n",
      "| 64  32   2    |\n",
      "|  4   2       2|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 100: right, reward: 20, total reward: 840\n",
      "-----------------\n",
      "|  2      16  16|\n",
      "|      2  64   4|\n",
      "|     64  32   2|\n",
      "|          4   4|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 101: left, reward: 40, total reward: 880\n",
      "-----------------\n",
      "|  2  32        |\n",
      "|  2  64   4   2|\n",
      "| 64  32   2    |\n",
      "|  8            |\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 102: down, reward: 4, total reward: 884\n",
      "-----------------\n",
      "|               |\n",
      "|  4  32   2    |\n",
      "| 64  64   4    |\n",
      "|  8  32   2   2|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 103: right, reward: 132, total reward: 1016\n",
      "-----------------\n",
      "|               |\n",
      "|      4  32   2|\n",
      "|      2 128   4|\n",
      "|      8  32   4|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 104: left, reward: 0, total reward: 1016\n",
      "-----------------\n",
      "|          2    |\n",
      "|  4  32   2    |\n",
      "|  2 128   4    |\n",
      "|  8  32   4    |\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 105: down, reward: 12, total reward: 1028\n",
      "-----------------\n",
      "|      2        |\n",
      "|  4  32        |\n",
      "|  2 128   4    |\n",
      "|  8  32   8    |\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 106: down, reward: 0, total reward: 1028\n",
      "-----------------\n",
      "|      2        |\n",
      "|  4  32        |\n",
      "|  2 128   4    |\n",
      "|  8  32   8    |\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 107: up, reward: 0, total reward: 1028\n",
      "-----------------\n",
      "|  4   2   4    |\n",
      "|  2  32   8    |\n",
      "|  8 128       2|\n",
      "|     32        |\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 108: left, reward: 0, total reward: 1028\n",
      "-----------------\n",
      "|  4   2   4    |\n",
      "|  2  32   8    |\n",
      "|  8 128   2    |\n",
      "| 32           2|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 109: right, reward: 0, total reward: 1028\n",
      "-----------------\n",
      "|      4   2   4|\n",
      "|  2   2  32   8|\n",
      "|      8 128   2|\n",
      "|         32   2|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 110: down, reward: 4, total reward: 1032\n",
      "-----------------\n",
      "|          2    |\n",
      "|  4   4  32   4|\n",
      "|      2 128   8|\n",
      "|  2   8  32   4|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 111: down, reward: 0, total reward: 1032\n",
      "-----------------\n",
      "|          2    |\n",
      "|  2   4  32   4|\n",
      "|  4   2 128   8|\n",
      "|  2   8  32   4|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 112: left, reward: 0, total reward: 1032\n",
      "-----------------\n",
      "|  2       2    |\n",
      "|  2   4  32   4|\n",
      "|  4   2 128   8|\n",
      "|  2   8  32   4|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 113: down, reward: 4, total reward: 1036\n",
      "-----------------\n",
      "|          2   2|\n",
      "|  4   4  32   4|\n",
      "|  4   2 128   8|\n",
      "|  2   8  32   4|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 114: left, reward: 12, total reward: 1048\n",
      "-----------------\n",
      "|  4            |\n",
      "|  8  32   4   2|\n",
      "|  4   2 128   8|\n",
      "|  2   8  32   4|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 115: up, reward: 0, total reward: 1048\n",
      "-----------------\n",
      "|  4  32   4   2|\n",
      "|  8   2 128   8|\n",
      "|  4   8  32   4|\n",
      "|  2       4    |\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 116: right, reward: 0, total reward: 1048\n",
      "-----------------\n",
      "|  4  32   4   2|\n",
      "|  8   2 128   8|\n",
      "|  4   8  32   4|\n",
      "|  2       2   4|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 117: down, reward: 8, total reward: 1056\n",
      "-----------------\n",
      "|  4       4   2|\n",
      "|  8  32 128   2|\n",
      "|  4   2  32   8|\n",
      "|  2   8   2   8|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 118: left, reward: 8, total reward: 1064\n",
      "-----------------\n",
      "|  8   2   2    |\n",
      "|  8  32 128   2|\n",
      "|  4   2  32   8|\n",
      "|  2   8   2   8|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 119: right, reward: 4, total reward: 1068\n",
      "-----------------\n",
      "|  2       8   4|\n",
      "|  8  32 128   2|\n",
      "|  4   2  32   8|\n",
      "|  2   8   2   8|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 120: left, reward: 0, total reward: 1068\n",
      "-----------------\n",
      "|  2   8   4   2|\n",
      "|  8  32 128   2|\n",
      "|  4   2  32   8|\n",
      "|  2   8   2   8|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 121: left, reward: 0, total reward: 1068\n",
      "-----------------\n",
      "|  2   8   4   2|\n",
      "|  8  32 128   2|\n",
      "|  4   2  32   8|\n",
      "|  2   8   2   8|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 122: left, reward: 0, total reward: 1068\n",
      "-----------------\n",
      "|  2   8   4   2|\n",
      "|  8  32 128   2|\n",
      "|  4   2  32   8|\n",
      "|  2   8   2   8|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 123: down, reward: 20, total reward: 1088\n",
      "-----------------\n",
      "|  2   8   4   2|\n",
      "|  8  32 128    |\n",
      "|  4   2  32   4|\n",
      "|  2   8   2  16|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 124: left, reward: 0, total reward: 1088\n",
      "-----------------\n",
      "|  2   8   4   2|\n",
      "|  8  32 128    |\n",
      "|  4   2  32   4|\n",
      "|  2   8   2  16|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 125: left, reward: 0, total reward: 1088\n",
      "-----------------\n",
      "|  2   8   4   2|\n",
      "|  8  32 128    |\n",
      "|  4   2  32   4|\n",
      "|  2   8   2  16|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 126: up, reward: 0, total reward: 1088\n",
      "-----------------\n",
      "|  2   8   4   2|\n",
      "|  8  32 128   4|\n",
      "|  4   2  32  16|\n",
      "|  2   8   2   2|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 127: right, reward: 4, total reward: 1092\n",
      "-----------------\n",
      "|  2   8   4   2|\n",
      "|  8  32 128   4|\n",
      "|  4   2  32  16|\n",
      "|  2   2   8   4|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 128: right, reward: 4, total reward: 1096\n",
      "-----------------\n",
      "|  2   8   4   2|\n",
      "|  8  32 128   4|\n",
      "|  4   2  32  16|\n",
      "|  2   4   8   4|\n",
      "-----------------\n",
      "\n",
      "\n",
      "Move 129: left, reward: 0, total reward: 1096\n",
      "-----------------\n",
      "|          2    |\n",
      "|               |\n",
      "|  2            |\n",
      "|               |\n",
      "-----------------\n",
      "\n",
      "\n",
      "Final score (sum of merges): 1096\n"
     ]
    }
   ],
   "source": [
    "# 4) play one game\n",
    "obs = vec_env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "k = 0\n",
    "dirr = {\n",
    "    0 : \"up\",\n",
    "    1 : \"down\",\n",
    "    2 : \"left\",\n",
    "    3 : \"right\",\n",
    "}\n",
    "env = make_env()\n",
    "# vec_env_play = make_env_play()\n",
    "re=nn.ReLU()\n",
    "while not done:\n",
    "    k += 1\n",
    "    # am = get_action_masks(env)\n",
    "    action, _ = model.predict(obs, deterministic=False)\n",
    "    obs, reward, done, _ = vec_env.step(action)\n",
    "    reward=max(int(reward[0]),0)\n",
    "    total_reward += reward\n",
    "    print(f\"Move {k}: {dirr[action[0]]}, reward: {reward}, total reward: {total_reward}\")\n",
    "    vec_env.render()\n",
    "    # print(action)\n",
    "    time.sleep(0.2)\n",
    "print(\"Final score (sum of merges):\", total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c44899f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1033.5 1044.0 482.36807 2532.0 256.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zy/tjrmvbjx24dcxfzdxg41rr400000gp/T/ipykernel_77553/2153850915.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  obs=torch.tensor(observations, dtype=int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1160.0 1126.0 529.5431993709295 2856.0 316.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def do_random():\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    k = 0\n",
    "    while not done:\n",
    "        k += 1\n",
    "        # am = get_action_masks(env)\n",
    "        action = [random.randint(0, 3)]\n",
    "        _, reward, done, _ = vec_env.step(action)\n",
    "        total_reward += max(reward[0],0)\n",
    "    return total_reward\n",
    "def do_thing():\n",
    "    obs = vec_env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    k = 0\n",
    "    while not done:\n",
    "        k += 1\n",
    "        # am = get_action_masks(env)\n",
    "        action, _ = model.predict(obs, deterministic=False)\n",
    "        obs, reward, done, _ = vec_env.step(action)\n",
    "        total_reward += max(reward[0],0)\n",
    "    return total_reward\n",
    "def do_wfnc(fnc):\n",
    "    l = np.array([fnc() for _ in range(40)])\n",
    "    plt.hist(l, bins=range(int(np.min(l)), int(np.max(l))+1, 2))\n",
    "    print(np.mean(l), np.median(l), np.std(l), np.max(l), np.min(l))\n",
    "do_wfnc(do_random)\n",
    "do_wfnc(do_thing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "af022152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1178.6 1072.0 570.0718200367389 3168.0 228.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zy/tjrmvbjx24dcxfzdxg41rr400000gp/T/ipykernel_77553/2153850915.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  obs=torch.tensor(observations, dtype=int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1138.32 1072.0 577.67566817376 3092.0 244.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def do_random():\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    k = 0\n",
    "    while not done:\n",
    "        k += 1\n",
    "        # am = get_action_masks(env)\n",
    "        action = [random.randint(0, 3)]\n",
    "        _, reward, done, _ = vec_env.step(action)\n",
    "        total_reward += max(reward[0],0)\n",
    "    return total_reward\n",
    "def do_thing():\n",
    "    obs = vec_env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    k = 0\n",
    "    while not done:\n",
    "        k += 1\n",
    "        # am = get_action_masks(env)\n",
    "        action, _ = model.predict(obs, deterministic=False)\n",
    "        obs, reward, done, _ = vec_env.step(action)\n",
    "        total_reward += max(reward[0],0)\n",
    "    return total_reward\n",
    "def do_wfnc(fnc):\n",
    "    l = np.array([fnc() for _ in range(100)])\n",
    "    plt.hist(l, bins=range(int(np.min(l)), int(np.max(l))+1, 2))\n",
    "    print(np.mean(l), np.median(l), np.std(l), np.max(l), np.min(l))\n",
    "do_wfnc(do_random)\n",
    "do_wfnc(do_thing)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
